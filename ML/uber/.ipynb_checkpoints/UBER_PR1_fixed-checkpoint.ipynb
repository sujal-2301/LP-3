{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f5cd659",
   "metadata": {},
   "source": [
    "\n",
    "# Uber Fare Prediction — Cleaned Practical Notebook\n",
    "\n",
    "**Tasks covered:**  \n",
    "1. Pre-process the dataset (datetime parsing, dtype fixes).  \n",
    "2. Identify outliers (IQR method) and optionally remove them.  \n",
    "3. Check correlations between features and target.  \n",
    "4. Implement Linear Regression and Random Forest Regression.  \n",
    "5. Evaluate models (R², RMSE, MAE) and compare results.\n",
    "\n",
    "**Notes:** This notebook expects a CSV file called `uber.csv` in the same folder. If your dataset has a different name, update the `DATA_PATH` variable below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c35d0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic imports and settings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_PATH = \"uber.csv\"  # change if your file has a different name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28cd542",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data (adjust path if needed)\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Loaded:\", DATA_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c609cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inspect columns and basic info\n",
    "print(df.columns.tolist())\n",
    "display(df.info())\n",
    "display(df.describe(include='all').T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1710de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop obvious useless columns if present (adjust to your dataset)\n",
    "for col in ['Unnamed: 0', 'key']:\n",
    "    if col in df.columns:\n",
    "        df.drop(columns=col, inplace=True)\n",
    "print(\"After dropping cols, shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11782d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parse pickup_datetime to datetime and extract month & hour as integer types\n",
    "if 'pickup_datetime' in df.columns:\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], errors='coerce')\n",
    "    df['month'] = df['pickup_datetime'].dt.month.astype('Int64')\n",
    "    df['hour'] = df['pickup_datetime'].dt.hour.astype('Int64')\n",
    "    # drop original if not needed\n",
    "    df.drop(columns=['pickup_datetime'], inplace=True)\n",
    "else:\n",
    "    print(\"No pickup_datetime column found; ensure your timestamp column is named 'pickup_datetime'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457224ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure numeric columns are numeric (coerce errors -> NaN)\n",
    "num_cols = ['fare_amount','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count']\n",
    "for c in num_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "# Show dtypes\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba93664",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Single, vectorized haversine distance function\n",
    "def haversine_vectorized(lon1, lat1, lon2, lat2):\n",
    "    # convert to radians\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, (lon1, lat1, lon2, lat2))\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6371 * c\n",
    "    return km\n",
    "\n",
    "# Add distance column if coordinates present\n",
    "coord_cols = ['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude']\n",
    "if all(col in df.columns for col in coord_cols):\n",
    "    df['distance_km'] = haversine_vectorized(df['pickup_longitude'].values,\n",
    "                                            df['pickup_latitude'].values,\n",
    "                                            df['dropoff_longitude'].values,\n",
    "                                            df['dropoff_latitude'].values)\n",
    "    print(\"Added distance_km column.\")\n",
    "else:\n",
    "    print(\"Coordinate columns missing; distance not computed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Handle missing or suspicious values\n",
    "# passenger_count: replace 0 or very large numbers with median of realistic values (1..6)\n",
    "if 'passenger_count' in df.columns:\n",
    "    med_pc = df.loc[df['passenger_count'].between(1,6),'passenger_count'].median()\n",
    "    df['passenger_count'] = df['passenger_count'].apply(lambda x: med_pc if pd.isna(x) or x==0 or x>6 else x)\n",
    "\n",
    "# fare_amount: replace non-positive fares with median positive fare\n",
    "if 'fare_amount' in df.columns:\n",
    "    med_fare = df.loc[df['fare_amount']>0,'fare_amount'].median()\n",
    "    df['fare_amount'] = df['fare_amount'].apply(lambda x: med_fare if pd.isna(x) or x<=0 else x)\n",
    "\n",
    "# Drop rows with essential NaNs (e.g., coordinates or fare)\n",
    "essential_cols = ['fare_amount','distance_km']\n",
    "for c in essential_cols:\n",
    "    if c in df.columns:\n",
    "        df = df[~df[c].isna()]\n",
    "\n",
    "print(\"After cleaning, shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dea5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Outlier detection using IQR for numeric columns of interest\n",
    "def remove_outliers_iqr(df, cols, k=1.5):\n",
    "    df_out = df.copy()\n",
    "    for col in cols:\n",
    "        if col in df_out.columns:\n",
    "            Q1 = df_out[col].quantile(0.25)\n",
    "            Q3 = df_out[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower = Q1 - k * IQR\n",
    "            upper = Q3 + k * IQR\n",
    "            # keep only rows within bounds\n",
    "            df_out = df_out[(df_out[col] >= lower) & (df_out[col] <= upper)]\n",
    "    return df_out\n",
    "\n",
    "numeric_check_cols = [c for c in ['fare_amount','distance_km','passenger_count','hour','month'] if c in df.columns]\n",
    "print(\"Numeric cols checked for outliers:\", numeric_check_cols)\n",
    "df_no_out = remove_outliers_iqr(df, numeric_check_cols, k=1.5)\n",
    "print(\"Shape before outlier removal:\", df.shape, \"after:\", df_no_out.shape)\n",
    "# You can switch to df = df_no_out if you want to remove outliers permanently\n",
    "# df = df_no_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b970d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Correlation heatmap (using numeric columns)\n",
    "plt.figure(figsize=(8,6))\n",
    "corr_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation matrix (numeric features)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e437facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare features and target\n",
    "# Choose a reasonable set of features\n",
    "features = []\n",
    "for c in ['distance_km','passenger_count','hour','month','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude']:\n",
    "    if c in df.columns:\n",
    "        features.append(c)\n",
    "print(\"Using features:\", features)\n",
    "\n",
    "X = df[features].copy()\n",
    "y = df['fare_amount'].copy()\n",
    "\n",
    "# Fill any remaining NaNs in X with median\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "print(\"Train/Test sizes:\", X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6f6260",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scale numeric features (important for linear models)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7e4ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "\n",
    "print(\"Linear Regression — R2: {:.4f}, RMSE: {:.4f}, MAE: {:.4f}\".format(r2_lr, rmse_lr, mae_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62820ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Random Forest Regression\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)  # tree-based models don't require scaled features\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest — R2: {:.4f}, RMSE: {:.4f}, MAE: {:.4f}\".format(r2_rf, rmse_rf, mae_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d39cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Comparison table\n",
    "results = pd.DataFrame({\n",
    "    'model': ['LinearRegression','RandomForest'],\n",
    "    'r2': [r2_lr, r2_rf],\n",
    "    'rmse': [rmse_lr, rmse_rf],\n",
    "    'mae': [mae_lr, mae_rf]\n",
    "})\n",
    "display(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f7f65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature importances from Random Forest (if available)\n",
    "if hasattr(rf, 'feature_importances_'):\n",
    "    importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "    print(\"Feature importances (Random Forest):\")\n",
    "    display(importances)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    importances.plot(kind='bar')\n",
    "    plt.title('Feature importances')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bd34b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save cleaned sample of dataframe and brief instructions\n",
    "df.sample(5)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
