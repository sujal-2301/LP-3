{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8267503",
   "metadata": {},
   "source": [
    "\n",
    "# Gradient Descent: Find Local Minima of \\( y = (x + 3)^2 \\)\n",
    "\n",
    "**Task:** Implement Gradient Descent to find the local minimum of the function \\( y=(x+3)^2 \\) starting from the point \\( x=2 \\).\n",
    "\n",
    "**Theory (very short):**\n",
    "- Gradient Descent update:  \\( x_{t+1} = x_t - \\eta \\, f'(x_t) \\)\n",
    "- For \\( f(x)=(x+3)^2 \\), the derivative is \\( f'(x)=2(x+3) \\).\n",
    "- The minimum of this convex quadratic is at \\( x=-3 \\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a2a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a41f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define function and derivative\n",
    "def f(x):\n",
    "    return (x + 3)**2\n",
    "\n",
    "def fprime(x):\n",
    "    return 2 * (x + 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b674b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradient_descent(fprime, x0, lr=0.1, tol=1e-8, max_iter=1000):\n",
    "    \"\"\"Simple 1D gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        fprime: derivative function\n",
    "        x0: starting point\n",
    "        lr: learning rate (eta)\n",
    "        tol: stop if successive x changes are smaller than this\n",
    "        max_iter: safety cap on iterations\n",
    "    Returns:\n",
    "        x_best: final x\n",
    "        history: list of x values visited (including start and last)\n",
    "        iters: number of performed iterations\n",
    "    \"\"\"\n",
    "    x = float(x0)\n",
    "    history = [x]\n",
    "    for t in range(max_iter):\n",
    "        grad = fprime(x)\n",
    "        x_new = x - lr * grad\n",
    "        history.append(x_new)\n",
    "        if abs(x_new - x) < tol or abs(grad) < tol:\n",
    "            return x_new, history, t + 1\n",
    "        x = x_new\n",
    "    return x, history, max_iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe00364",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run gradient descent for the given example\n",
    "x0 = 2.0           # start from x = 2\n",
    "eta = 0.1          # learning rate\n",
    "x_star, path, steps = gradient_descent(fprime, x0, lr=eta)\n",
    "\n",
    "print(f\"Start x0 = {x0}\")\n",
    "print(f\"Learning rate = {eta}\")\n",
    "print(f\"Converged in {steps} steps\") \n",
    "print(f\"Estimated minimizer x* = {x_star:.10f}\")\n",
    "print(f\"f(x*) = {f(x_star):.10f}\")\n",
    "print(f\"True minimizer is at x = -3.0; error = {abs(x_star + 3):.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc6eea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize the function and the descent path\n",
    "xs = np.linspace(-8, 4, 300)\n",
    "ys = (xs + 3)**2\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xs, ys)\n",
    "plt.scatter(path, [(p + 3)**2 for p in path])\n",
    "plt.title('Gradient Descent on f(x) = (x + 3)^2')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce2b94f",
   "metadata": {},
   "source": [
    "\n",
    "### Notes\n",
    "- If it diverges, reduce `lr` (learning rate). For this convex function, `lr = 0.1` works well.\n",
    "- Stopping criteria: tiny change in `x` or tiny gradient value.\n",
    "- You can experiment with different `x0` and `lr` to see how the path changes.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
